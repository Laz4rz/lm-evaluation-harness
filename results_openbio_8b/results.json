{
  "results": {
    "polish": {
      "acc_norm,none": 0.43718949609652236,
      "acc_norm_stderr,none": 0.003004288852547825,
      "exact_match,none": 0.0,
      "exact_match_stderr,none": 0.0,
      "f1,score-first": 0.1342219394812514,
      "f1_stderr,score-first": "N/A",
      "exact_match,score-first": 0.41397297083695567,
      "exact_match_stderr,score-first": 0.0038076609666961242,
      "acc,none": 0.4506348079804432,
      "acc_stderr,none": 0.0030292830363586017,
      "f1,none": 0.3094389477847203,
      "f1_stderr,none": "N/A",
      "levenshtein,none": 0.3870265563778842,
      "levenshtein_stderr,none": "N/A",
      "alias": "polish"
    },
    "polemo2_in": {
      "exact_match,score-first": 0.5858725761772853,
      "exact_match_stderr,score-first": 0.018344291710620065,
      "alias": " - polemo2_in"
    },
    "polemo2_in_multiple_choice": {
      "acc,none": 0.2188365650969529,
      "acc_stderr,none": 0.015397962780823109,
      "acc_norm,none": 0.20083102493074792,
      "acc_norm_stderr,none": 0.014919940038662937,
      "alias": " - polemo2_in_multiple_choice"
    },
    "polemo2_out": {
      "exact_match,score-first": 0.5121457489878543,
      "exact_match_stderr,score-first": 0.022512222543699473,
      "alias": " - polemo2_out"
    },
    "polemo2_out_multiple_choice": {
      "acc,none": 0.2550607287449393,
      "acc_stderr,none": 0.019631739079978186,
      "acc_norm,none": 0.2773279352226721,
      "acc_norm_stderr,none": 0.020162481553849795,
      "alias": " - polemo2_out_multiple_choice"
    },
    "polish_8tags_multiple_choice": {
      "acc,none": 0.43755718206770355,
      "acc_stderr,none": 0.0075035398416525385,
      "acc_norm,none": 0.358188472095151,
      "acc_norm_stderr,none": 0.007252189700065307,
      "alias": " - polish_8tags_multiple_choice"
    },
    "polish_8tags_regex": {
      "exact_match,score-first": 0.6713174748398902,
      "exact_match_stderr,score-first": 0.007104963976857329,
      "alias": " - polish_8tags_regex"
    },
    "polish_belebele_mc": {
      "acc,none": 0.6511111111111111,
      "acc_stderr,none": 0.01589612527552071,
      "acc_norm,none": 0.6511111111111111,
      "acc_norm_stderr,none": 0.01589612527552071,
      "alias": " - polish_belebele_mc"
    },
    "polish_belebele_regex": {
      "exact_match,score-first": 0.5822222222222222,
      "exact_match_stderr,score-first": 0.01644891326773528,
      "alias": " - polish_belebele_regex"
    },
    "polish_cbd_multiple_choice": {
      "acc,none": 0.767,
      "acc_stderr,none": 0.013374972519220056,
      "f1,none": 0.15821921146254692,
      "f1_stderr,none": "N/A",
      "acc_norm,none": 0.794,
      "acc_norm_stderr,none": 0.012795613612786541,
      "alias": " - polish_cbd_multiple_choice"
    },
    "polish_cbd_regex": {
      "exact_match,score-first": 0.224,
      "exact_match_stderr,score-first": 0.013190830072364471,
      "f1,score-first": 0.11701222069970589,
      "f1_stderr,score-first": "N/A",
      "alias": " - polish_cbd_regex"
    },
    "polish_dyk_multiple_choice": {
      "acc,none": 0.18561710398445092,
      "acc_stderr,none": 0.012126256792054294,
      "f1,none": 0.2922297297297297,
      "f1_stderr,none": "N/A",
      "acc_norm,none": 0.18561710398445092,
      "acc_norm_stderr,none": 0.012126256792054294,
      "alias": " - polish_dyk_multiple_choice"
    },
    "polish_dyk_regex": {
      "exact_match,score-first": 0.1924198250728863,
      "exact_match_stderr,score-first": 0.012294791690621143,
      "f1,score-first": 0.2915601023017903,
      "f1_stderr,score-first": "N/A",
      "alias": " - polish_dyk_regex"
    },
    "polish_klej_ner_multiple_choice": {
      "acc,none": 0.27988338192419826,
      "acc_stderr,none": 0.009898578401658567,
      "acc_norm,none": 0.2706511175898931,
      "acc_norm_stderr,none": 0.00979615002152182,
      "alias": " - polish_klej_ner_multiple_choice"
    },
    "polish_klej_ner_regex": {
      "exact_match,score-first": 0.28668610301263364,
      "exact_match_stderr,score-first": 0.009970719928037169,
      "alias": " - polish_klej_ner_regex"
    },
    "polish_polqa_closed_book": {
      "exact_match,none": 0.0,
      "exact_match_stderr,none": 0.0,
      "levenshtein,none": 0.245067497403946,
      "levenshtein_stderr,none": "N/A",
      "alias": " - polish_polqa_closed_book"
    },
    "polish_polqa_open_book": {
      "exact_match,none": 0.0,
      "exact_match_stderr,none": 0.0,
      "levenshtein,none": 0.4100877192982456,
      "levenshtein_stderr,none": "N/A",
      "alias": " - polish_polqa_open_book"
    },
    "polish_polqa_reranking_multiple_choice": {
      "acc,none": 0.4980722322763396,
      "acc_stderr,none": 0.004435352828634602,
      "acc_norm,none": 0.4980722322763396,
      "acc_norm_stderr,none": 0.004435352828634602,
      "alias": " - polish_polqa_reranking_multiple_choice"
    },
    "polish_ppc_multiple_choice": {
      "acc,none": 0.453,
      "acc_stderr,none": 0.015749255189977596,
      "acc_norm,none": 0.453,
      "acc_norm_stderr,none": 0.015749255189977596,
      "alias": " - polish_ppc_multiple_choice"
    },
    "polish_ppc_regex": {
      "exact_match,score-first": 0.091,
      "exact_match_stderr,score-first": 0.009099549538400241,
      "alias": " - polish_ppc_regex"
    },
    "polish_psc_multiple_choice": {
      "acc,none": 0.3051948051948052,
      "acc_stderr,none": 0.01403176314810364,
      "f1,none": 0.466143977191732,
      "f1_stderr,none": "N/A",
      "acc_norm,none": 0.3051948051948052,
      "acc_norm_stderr,none": 0.01403176314810364,
      "alias": " - polish_psc_multiple_choice"
    },
    "polish_psc_regex": {
      "exact_match,score-first": 0.0,
      "exact_match_stderr,score-first": 0.0,
      "f1,score-first": 0.0,
      "f1_stderr,score-first": "N/A",
      "alias": " - polish_psc_regex"
    }
  },
  "groups": {
    "polish": {
      "acc_norm,none": 0.43718949609652236,
      "acc_norm_stderr,none": 0.003004288852547825,
      "exact_match,none": 0.0,
      "exact_match_stderr,none": 0.0,
      "f1,score-first": 0.1342219394812514,
      "f1_stderr,score-first": "N/A",
      "exact_match,score-first": 0.41397297083695567,
      "exact_match_stderr,score-first": 0.0038076609666961242,
      "acc,none": 0.4506348079804432,
      "acc_stderr,none": 0.0030292830363586017,
      "f1,none": 0.3094389477847203,
      "f1_stderr,none": "N/A",
      "levenshtein,none": 0.3870265563778842,
      "levenshtein_stderr,none": "N/A",
      "alias": "polish"
    }
  },
  "group_subtasks": {
    "polish": [
      "polish_polqa_reranking_multiple_choice",
      "polish_polqa_open_book",
      "polish_polqa_closed_book",
      "polish_klej_ner_regex",
      "polish_klej_ner_multiple_choice",
      "polish_cbd_regex",
      "polish_cbd_multiple_choice",
      "polish_psc_regex",
      "polish_psc_multiple_choice",
      "polish_ppc_regex",
      "polish_ppc_multiple_choice",
      "polish_dyk_regex",
      "polish_dyk_multiple_choice",
      "polish_belebele_regex",
      "polish_belebele_mc",
      "polish_8tags_regex",
      "polish_8tags_multiple_choice",
      "polemo2_out_multiple_choice",
      "polemo2_in_multiple_choice",
      "polemo2_out",
      "polemo2_in"
    ]
  },
  "configs": {
    "polemo2_in": {
      "task": "polemo2_in",
      "group": [
        "polemo2"
      ],
      "dataset_path": "allegro/klej-polemo2-in",
      "training_split": "train",
      "validation_split": "validation",
      "test_split": "test",
      "doc_to_text": "Opinia: \"{{sentence}}\"\nOkreśl sentyment podanej opinii. Możliwe odpowiedzi:\nA - Neutralny\nB - Negatywny\nC - Pozytywny\nD - Niejednoznaczny\nPrawidłowa odpowiedź:",
      "doc_to_target": "{{{'__label__meta_zero': 'A', '__label__meta_minus_m': 'B', '__label__meta_plus_m': 'C', '__label__meta_amb': 'D'}.get(target)}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "hf_evaluate": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          ".",
          ","
        ],
        "do_sample": false,
        "temperature": 0.0,
        "max_gen_toks": 50
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "score-first",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "(\\b[ABCD]\\b)"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": true,
      "doc_to_decontamination_query": "{{sentence}}",
      "metadata": {
        "version": 1.0
      }
    },
    "polemo2_in_multiple_choice": {
      "task": "polemo2_in_multiple_choice",
      "group": [
        "polemo2_mc"
      ],
      "dataset_path": "allegro/klej-polemo2-in",
      "training_split": "train",
      "validation_split": "validation",
      "test_split": "test",
      "doc_to_text": "Opinia: \"{{sentence}}\"\nOkreśl sentyment podanej opinii: Neutralny, Negatywny, Pozytywny, Niejednoznaczny.\nSentyment:",
      "doc_to_target": "{{['__label__meta_zero', '__label__meta_minus_m', '__label__meta_plus_m', '__label__meta_amb'].index(target)}}",
      "doc_to_choice": [
        "Neutralny",
        "Negatywny",
        "Pozytywny",
        "Niejednoznaczny"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": true,
      "doc_to_decontamination_query": "{{sentence}}"
    },
    "polemo2_out": {
      "task": "polemo2_out",
      "group": [
        "polemo2"
      ],
      "dataset_path": "allegro/klej-polemo2-out",
      "training_split": "train",
      "validation_split": "validation",
      "test_split": "test",
      "doc_to_text": "Opinia: \"{{sentence}}\"\nOkreśl sentyment podanej opinii. Możliwe odpowiedzi:\nA - Neutralny\nB - Negatywny\nC - Pozytywny\nD - Niejednoznaczny\nPrawidłowa odpowiedź:",
      "doc_to_target": "{{{'__label__meta_zero': 'A', '__label__meta_minus_m': 'B', '__label__meta_plus_m': 'C', '__label__meta_amb': 'D'}.get(target)}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "hf_evaluate": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          ".",
          ","
        ],
        "do_sample": false,
        "temperature": 0.0,
        "max_gen_toks": 50
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "score-first",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "(\\b[ABCD]\\b)"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": true,
      "doc_to_decontamination_query": "{{sentence}}",
      "metadata": {
        "version": 1.0
      }
    },
    "polemo2_out_multiple_choice": {
      "task": "polemo2_out_multiple_choice",
      "group": [
        "polemo2_mc"
      ],
      "dataset_path": "allegro/klej-polemo2-out",
      "training_split": "train",
      "validation_split": "validation",
      "test_split": "test",
      "doc_to_text": "Opinia: \"{{sentence}}\"\nOkreśl sentyment podanej opinii: Neutralny, Negatywny, Pozytywny, Niejednoznaczny.\nSentyment:",
      "doc_to_target": "{{['__label__meta_zero', '__label__meta_minus_m', '__label__meta_plus_m', '__label__meta_amb'].index(target)}}",
      "doc_to_choice": [
        "Neutralny",
        "Negatywny",
        "Pozytywny",
        "Niejednoznaczny"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": true,
      "doc_to_decontamination_query": "{{sentence}}"
    },
    "polish_8tags_multiple_choice": {
      "task": "polish_8tags_multiple_choice",
      "dataset_path": "sdadas/8tags",
      "training_split": "train",
      "test_split": "test",
      "fewshot_split": "train",
      "doc_to_text": "Tytuł: \"{{sentence}}\"\nDo podanego tytułu przyporządkuj jedną najlepiej pasującą kategorię z podanych: Film, Historia, Jedzenie, Medycyna, Motoryzacja, Praca, Sport, Technologie.\nKategoria:",
      "doc_to_target": "{{label|int}}",
      "doc_to_choice": [
        "Film",
        "Historia",
        "Jedzenie",
        "Medycyna",
        "Motoryzacja",
        "Praca",
        "Sport",
        "Technologie"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": true,
      "doc_to_decontamination_query": "{{sentence}}"
    },
    "polish_8tags_regex": {
      "task": "polish_8tags_regex",
      "dataset_path": "sdadas/8tags",
      "training_split": "train",
      "validation_split": "validation",
      "test_split": "test",
      "doc_to_text": "Tytuł: \"{{sentence}}\"\nPytanie: jaka kategoria najlepiej pasuje do podanego tytułu?\nMożliwe odpowiedzi:\nA - film\nB - historia\nC - jedzenie\nD - medycyna\nE - motoryzacja\nF - praca\nG - sport\nH - technologie\nPrawidłowa odpowiedź:",
      "doc_to_target": "{{{0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F', 6: 'G', 7: 'H'}.get(label)}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          ".",
          ","
        ],
        "do_sample": false,
        "temperature": 0.0,
        "max_gen_toks": 50
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "score-first",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "(\\b[ABCDEFGH]\\b)"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": true,
      "doc_to_decontamination_query": "{{sentence}}"
    },
    "polish_belebele_mc": {
      "task": "polish_belebele_mc",
      "dataset_path": "facebook/belebele",
      "test_split": "pol_Latn",
      "fewshot_split": "pol_Latn",
      "doc_to_text": "Fragment: \"{{flores_passage}}\"\nPytanie: \"{{question}}\"\nMożliwe odpowiedzi:\nA - {{mc_answer1}}\nB - {{mc_answer2}}\nC - {{mc_answer3}}\nD - {{mc_answer4}}\nPrawidłowa odpowiedź:",
      "doc_to_target": "{{['1', '2', '3', '4'].index(correct_answer_num)}}",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": true,
      "doc_to_decontamination_query": "{{question}}",
      "metadata": {
        "version": 0.0
      }
    },
    "polish_belebele_regex": {
      "task": "polish_belebele_regex",
      "dataset_path": "facebook/belebele",
      "test_split": "pol_Latn",
      "doc_to_text": "Fragment: \"{{flores_passage}}\"\nPytanie: \"{{question}}\"\nMożliwe odpowiedzi:\nA - {{mc_answer1}}\nB - {{mc_answer2}}\nC - {{mc_answer3}}\nD - {{mc_answer4}}\nPrawidłowa odpowiedź:",
      "doc_to_target": "{{{0: 'A', 1: 'B', 2: 'C', 3: 'D'}.get(correct_answer_num|int - 1)}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          ".",
          ","
        ],
        "do_sample": false,
        "temperature": 0.0,
        "max_gen_toks": 50
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "score-first",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "(\\b[ABCD]\\b)"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": true,
      "doc_to_decontamination_query": "{{flores_passage}} {{question}} {{mc_answer1}} {{mc_answer2}} {{mc_answer3}} {{mc_answer4}}"
    },
    "polish_cbd_multiple_choice": {
      "task": "polish_cbd_multiple_choice",
      "dataset_path": "ptaszynski/PolishCyberbullyingDataset",
      "training_split": "train",
      "test_split": "test",
      "doc_to_text": "Wypowiedź: \"{{TEXT}}\"\nDo podanej wypowiedzi przyporządkuj jedną, najlepiej pasującą kategorię z podanych: nieszkodliwa, szyderstwo, obelga, insynuacja, groźba, molestowanie.\nKategoria:",
      "doc_to_target": "{{{'szyderstwo': 1, 'obelga': 2, 'insynuacja': 3, 'grozba': 4, 'molestowanie': 5}.get(CATEGORIES, 0)}}",
      "doc_to_choice": [
        "nieszkodliwa",
        "szyderstwo",
        "obelga",
        "insynuacja",
        "groźba",
        "molestowanie"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "def f1(predictions, references):\n    _prediction = predictions[0]\n    _reference = references[0]\n    string_label = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]\n    reference = string_label.index(_reference)\n    prediction = (\n        string_label.index(_prediction)\n        if _prediction in string_label\n        else 0\n    )\n\n    return (prediction, reference)\n",
          "aggregation": "def agg_f1_macro(items):\n    predictions, references = zip(*items)\n    references, predictions = np.asarray(references), np.asarray(predictions)\n\n    return sklearn.metrics.f1_score(references, predictions, average='macro')\n",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": true,
      "doc_to_decontamination_query": "{{TEXT}}"
    },
    "polish_cbd_regex": {
      "task": "polish_cbd_regex",
      "dataset_path": "ptaszynski/PolishCyberbullyingDataset",
      "training_split": "train",
      "test_split": "test",
      "doc_to_text": "Wypowiedź: \"{{TEXT}}\"\nPytanie: Jaka kategoria najlepiej pasuje do podanej wypowiedzi?\nMożliwe odpowiedzi:\nA - nieszkodliwa\nB - szyderstwo\nC - obelga\nD - insynuacja\nE - groźba\nF - molestowanie\nPrawidłowa odpowiedź:",
      "doc_to_target": "{{{'szyderstwo': 'B', 'obelga': 'C', 'insynuacja': 'D', 'grozba': 'E', 'molestowanie': 'F'}.get(CATEGORIES, 'A')}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "def f1(predictions, references):\n    _prediction = predictions[0]\n    _reference = references[0]\n    string_label = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]\n    reference = string_label.index(_reference)\n    prediction = (\n        string_label.index(_prediction)\n        if _prediction in string_label\n        else 0\n    )\n\n    return (prediction, reference)\n",
          "aggregation": "def agg_f1_macro(items):\n    predictions, references = zip(*items)\n    references, predictions = np.asarray(references), np.asarray(predictions)\n\n    return sklearn.metrics.f1_score(references, predictions, average='macro')\n",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          ".",
          ",",
          ";"
        ],
        "do_sample": false,
        "temperature": 0.0,
        "max_gen_toks": 50
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "score-first",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "(\\b[ABCDEF]\\b)"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": true,
      "doc_to_decontamination_query": "{{TEXT}}"
    },
    "polish_dyk_multiple_choice": {
      "task": "polish_dyk_multiple_choice",
      "dataset_path": "allegro/klej-dyk",
      "training_split": "train",
      "test_split": "test",
      "doc_to_text": "Pytanie: \"{{question}}\"\nSugerowana odpowiedź: \"{{answer}}\"\nPytanie: Czy sugerowana odpowiedź na zadane pytanie jest poprawna?\nOdpowiedz krótko \"Tak\" lub \"Nie\". Prawidłowa odpowiedź:",
      "doc_to_target": "{{target|int}}",
      "doc_to_choice": [
        "Nie",
        "Tak"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "def f1(predictions, references):\n    _prediction = predictions[0]\n    _reference = references[0]\n    string_label = [\"B\", \"C\"]\n    reference = string_label.index(_reference)\n    prediction = (\n        string_label.index(_prediction)\n        if _prediction in string_label\n        else 0\n    )\n\n    return (prediction, reference)\n",
          "aggregation": "def agg_f1(items):\n    predictions, references = zip(*items)\n    references, predictions = np.asarray(references), np.asarray(predictions)\n\n    return sklearn.metrics.f1_score(references, predictions)\n",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": true,
      "doc_to_decontamination_query": "{{question}} {{answer}}"
    },
    "polish_dyk_regex": {
      "task": "polish_dyk_regex",
      "dataset_path": "allegro/klej-dyk",
      "training_split": "train",
      "test_split": "test",
      "doc_to_text": "Pytanie: \"{{question}}\"\nSugerowana odpowiedź: \"{{answer}}\"\nCzy sugerowana odpowiedź na zadane pytanie jest poprawna? Możliwe opcje:\nA - brakuje sugerowanej odpowiedzi\nB - nie, sugerowana odpowiedź nie jest poprawna\nC - tak, sugerowana odpowiedź jest poprawna\nD - brakuje pytania\nPrawidłowa opcja:",
      "doc_to_target": "{{{0: 'A', 1: 'B', 2: 'C', 3: 'D'}.get(target|int + 1)}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "def f1(predictions, references):\n    _prediction = predictions[0]\n    _reference = references[0]\n    string_label = [\"B\", \"C\"]\n    reference = string_label.index(_reference)\n    prediction = (\n        string_label.index(_prediction)\n        if _prediction in string_label\n        else 0\n    )\n\n    return (prediction, reference)\n",
          "aggregation": "def agg_f1(items):\n    predictions, references = zip(*items)\n    references, predictions = np.asarray(references), np.asarray(predictions)\n\n    return sklearn.metrics.f1_score(references, predictions)\n",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          ".",
          ","
        ],
        "do_sample": false,
        "temperature": 0.0,
        "max_gen_toks": 50
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "score-first",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "(\\b[ABCD]\\b)"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": true,
      "doc_to_decontamination_query": "{{question}} {{answer}}"
    },
    "polish_klej_ner_multiple_choice": {
      "task": "polish_klej_ner_multiple_choice",
      "dataset_path": "allegro/klej-nkjp-ner",
      "training_split": "train",
      "validation_split": "validation",
      "test_split": "test",
      "fewshot_split": "train",
      "doc_to_text": "Zdanie: \"{{sentence}}\"\nJakiego rodzaju jest nazwana jednostka, jeżeli występuje w podanym zdaniu?\nMożliwe odpowiedzi: Brak nazwanej jednostki, Nazwa miejsca, Nazwa osoby, Nazwa organizacji, Czas, Nazwa geograficzna.\nRodzaj:",
      "doc_to_target": "{{{'noEntity': 0, 'placeName': 1, 'persName': 2, 'orgName': 3, 'time': 4, 'geogName': 5}.get(target)}}",
      "doc_to_choice": [
        "Brak nazwanej jednostki",
        "Nazwa miejsca",
        "Nazwa osoby",
        "Nazwa organizacji",
        "Czas",
        "Nazwa geograficzna"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": true,
      "doc_to_decontamination_query": "{{sentence}}"
    },
    "polish_klej_ner_regex": {
      "task": "polish_klej_ner_regex",
      "dataset_path": "allegro/klej-nkjp-ner",
      "training_split": "train",
      "validation_split": "validation",
      "test_split": "test",
      "doc_to_text": "Zdanie: \"{{sentence}}\"\nPytanie: Jakiego rodzaju jest nazwana jednostka, jeżeli występuje w podanym zdaniu?\nMożliwe odpowiedzi:\nA - Brak nazwanej jednostki\nB - Nazwa miejsca\nC - Nazwa osoby\nD - Nazwa organizacji\nE - Czas\nF - Nazwa geograficzna\nPrawidłowa odpowiedź:",
      "doc_to_target": "{{{'noEntity': 'A', 'placeName': 'B', 'persName': 'C', 'orgName': 'D', 'time': 'E', 'geogName': 'F'}.get(target)}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          ".",
          ",",
          ";"
        ],
        "do_sample": false,
        "temperature": 0.0,
        "max_gen_toks": 50
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "score-first",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "(\\b[ABCDEF]\\b)"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": true,
      "doc_to_decontamination_query": "{{sentence}}"
    },
    "polish_polqa_closed_book": {
      "task": "polish_polqa_closed_book",
      "dataset_path": "ipipan/polqa",
      "training_split": "train",
      "validation_split": "validation",
      "test_split": "test",
      "process_docs": "def process_docs_closed(dataset: datasets.Dataset):\n    def _helper(doc):\n      doc[\"answers\"] = ast.literal_eval(doc['answers'])\n      return doc\n\n    used = set()\n\n    return dataset.remove_columns(COLUMNS_TO_REMOVE).filter(lambda example: example[\"relevant\"] and example['question'] not in used and (used.add(example['question']) or True)).map(_helper)\n",
      "doc_to_text": "Pytanie: {{question}} \n Prawidłowa odpowiedź:",
      "doc_to_target": "answers",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "def levenshtein(predictions, references):\n    _prediction = predictions[0][0].lower()\n    prediction_number = get_number(_prediction)\n\n    _prediction = re.sub('\\.? ?(</s>)* ?$','',_prediction)\n\n    for reference in references:\n        reference_number = get_number(reference)\n\n        if reference_number is not None:\n            if reference_number == prediction_number:\n                return 1\n        else:\n            ld = distance(_prediction, reference.lower())\n            if ld<len(reference)/2:\n                return 1\n    return 0\n",
          "aggregation": "def agg_levenshtein(items):\n    return sum(items)/len(items)\n",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "\n",
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0,
        "max_gen_toks": 50
      },
      "repeats": 1,
      "should_decontaminate": true,
      "doc_to_decontamination_query": "{{question}}"
    },
    "polish_polqa_open_book": {
      "task": "polish_polqa_open_book",
      "dataset_path": "ipipan/polqa",
      "training_split": "train",
      "validation_split": "validation",
      "test_split": "test",
      "process_docs": "def process_docs_open(dataset: datasets.Dataset):\n    def _helper(doc):\n      doc[\"answers\"] = ast.literal_eval(doc['answers'])\n      return doc\n\n    used = set()\n\n    return dataset.remove_columns(COLUMNS_TO_REMOVE).filter(lambda example: example[\"relevant\"] and (example['passage_text'],example['question']) not in used and (used.add((example['passage_text'],example['question'])) or True)).map(_helper)\n",
      "doc_to_text": "Kontekst: {{passage_text}} \n Pytanie: {{question}} \n Prawidłowa odpowiedź:",
      "doc_to_target": "answers",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "def levenshtein(predictions, references):\n    _prediction = predictions[0][0].lower()\n    prediction_number = get_number(_prediction)\n\n    _prediction = re.sub('\\.? ?(</s>)* ?$','',_prediction)\n\n    for reference in references:\n        reference_number = get_number(reference)\n\n        if reference_number is not None:\n            if reference_number == prediction_number:\n                return 1\n        else:\n            ld = distance(_prediction, reference.lower())\n            if ld<len(reference)/2:\n                return 1\n    return 0\n",
          "aggregation": "def agg_levenshtein(items):\n    return sum(items)/len(items)\n",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "\n",
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0,
        "max_gen_toks": 50
      },
      "repeats": 1,
      "should_decontaminate": true,
      "doc_to_decontamination_query": "{{passage_text}} {{question}}"
    },
    "polish_polqa_reranking_multiple_choice": {
      "task": "polish_polqa_reranking_multiple_choice",
      "dataset_path": "ipipan/polqa",
      "training_split": "train",
      "validation_split": "validation",
      "test_split": "test",
      "process_docs": "def process_docs(dataset: datasets.Dataset):\n    def _helper(doc):\n      return doc\n\n    used = set()\n\n    return dataset.remove_columns(COLUMNS_TO_REMOVE).filter(lambda example: (example['passage_text'],example['question']) not in used and (used.add((example['passage_text'],example['question'])) or True)).map(_helper)\n",
      "doc_to_text": "Kontekst: {{passage_text}} \n Pytanie: {{question}} \n Czy kontekst jest relewantny dla pytania? \n Odpowiedz krótko \"Tak\" lub \"Nie\". Prawidłowa odpowiedź:",
      "doc_to_target": "{{relevant|int}}",
      "doc_to_choice": [
        "Nie",
        "Tak"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": true,
      "doc_to_decontamination_query": "{{passage_text}} {{question}}"
    },
    "polish_ppc_multiple_choice": {
      "task": "polish_ppc_multiple_choice",
      "dataset_path": "sdadas/ppc",
      "training_split": "train",
      "validation_split": "validation",
      "test_split": "test",
      "doc_to_text": "Zdanie A: \"{{sentence_A}}\"\nZdanie B: \"{{sentence_B}}\"\nPytanie: jaka jest zależność między zdaniami A i B? Możliwe odpowiedzi:\nA - znaczą dokładnie to samo\nB - mają podobne znaczenie\nC - mają różne znaczenie\nPrawidłowa odpowiedź:",
      "doc_to_target": "{{label|int - 1}}",
      "doc_to_choice": [
        "A",
        "B",
        "C"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": true,
      "doc_to_decontamination_query": "{{sentence_A}} {{sentence_B}}"
    },
    "polish_ppc_regex": {
      "task": "polish_ppc_regex",
      "dataset_path": "sdadas/ppc",
      "training_split": "train",
      "validation_split": "validation",
      "test_split": "test",
      "doc_to_text": "Zdanie A: \"{{sentence_A}}\"\nZdanie B: \"{{sentence_B}}\"\nPytanie: jaka jest zależność między zdaniami A i B? Możliwe odpowiedzi:\nA - wszystkie odpowiedzi poprawne\nB - znaczą dokładnie to samo\nC - mają podobne znaczenie\nD - mają różne znaczenie\nPrawidłowa odpowiedź:",
      "doc_to_target": "{{{0: 'A', 1: 'B', 2: 'C', 3: 'D'}.get(label|int)}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          ".",
          ","
        ],
        "do_sample": false,
        "temperature": 0.0,
        "max_gen_toks": 50
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "score-first",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "(\\b[ABCD]\\b)"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": true,
      "doc_to_decontamination_query": "{{sentence_A}} {{sentence_B}}"
    },
    "polish_psc_multiple_choice": {
      "task": "polish_psc_multiple_choice",
      "dataset_path": "allegro/klej-psc",
      "training_split": "train",
      "test_split": "test",
      "doc_to_text": "Tekst: \"{{extract_text}}\"\nPodsumowanie: \"{{summary_text}}\"\nPytanie: Czy podsumowanie dla podanego tekstu jest poprawne?\nOdpowiedz krótko \"Tak\" lub \"Nie\". Prawidłowa odpowiedź:",
      "doc_to_target": "{{label|int}}",
      "doc_to_choice": [
        "Nie",
        "Tak"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "def f1(predictions, references):\n    _prediction = predictions[0]\n    _reference = references[0]\n    string_label = [\"B\", \"C\"]\n    reference = string_label.index(_reference)\n    prediction = (\n        string_label.index(_prediction)\n        if _prediction in string_label\n        else 0\n    )\n\n    return (prediction, reference)\n",
          "aggregation": "def agg_f1(items):\n    predictions, references = zip(*items)\n    references, predictions = np.asarray(references), np.asarray(predictions)\n\n    return sklearn.metrics.f1_score(references, predictions)\n",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": true,
      "doc_to_decontamination_query": "{{extract_text}} {{summary_text}}"
    },
    "polish_psc_regex": {
      "task": "polish_psc_regex",
      "dataset_path": "allegro/klej-psc",
      "training_split": "train",
      "test_split": "test",
      "doc_to_text": "Fragment 1: \"{{extract_text}}\"\nFragment 2: \"{{summary_text}}\"\nPytanie: jaka jest zależność między fragmentami 1 i 2?\nMożliwe odpowiedzi:\nA - wszystkie odpowiedzi poprawne\nB - dotyczą tego samego artykułu\nC - dotyczą różnych artykułów\nD - brak poprawnej odpowiedzi\nPrawidłowa odpowiedź:",
      "doc_to_target": "{{{0: 'A', 1: 'B', 2: 'C', 3: 'D'}.get(label|int + 1)}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "def f1(predictions, references):\n    _prediction = predictions[0]\n    _reference = references[0]\n    string_label = [\"B\", \"C\"]\n    reference = string_label.index(_reference)\n    prediction = (\n        string_label.index(_prediction)\n        if _prediction in string_label\n        else 0\n    )\n\n    return (prediction, reference)\n",
          "aggregation": "def agg_f1(items):\n    predictions, references = zip(*items)\n    references, predictions = np.asarray(references), np.asarray(predictions)\n\n    return sklearn.metrics.f1_score(references, predictions)\n",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          ".",
          ","
        ],
        "do_sample": false,
        "temperature": 0.0,
        "max_gen_toks": 50
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "score-first",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "(\\b[ABCD]\\b)"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": true,
      "doc_to_decontamination_query": "{{extract_text}} {{summary_text}}"
    }
  },
  "versions": {
    "polemo2_in": 1.0,
    "polemo2_in_multiple_choice": "Yaml",
    "polemo2_out": 1.0,
    "polemo2_out_multiple_choice": "Yaml",
    "polish_8tags_multiple_choice": "Yaml",
    "polish_8tags_regex": "Yaml",
    "polish_belebele_mc": 0.0,
    "polish_belebele_regex": "Yaml",
    "polish_cbd_multiple_choice": "Yaml",
    "polish_cbd_regex": "Yaml",
    "polish_dyk_multiple_choice": "Yaml",
    "polish_dyk_regex": "Yaml",
    "polish_klej_ner_multiple_choice": "Yaml",
    "polish_klej_ner_regex": "Yaml",
    "polish_polqa_closed_book": "Yaml",
    "polish_polqa_open_book": "Yaml",
    "polish_polqa_reranking_multiple_choice": "Yaml",
    "polish_ppc_multiple_choice": "Yaml",
    "polish_ppc_regex": "Yaml",
    "polish_psc_multiple_choice": "Yaml",
    "polish_psc_regex": "Yaml"
  },
  "n-shot": {
    "polemo2_in": 0,
    "polemo2_in_multiple_choice": 0,
    "polemo2_out": 0,
    "polemo2_out_multiple_choice": 0,
    "polish": 0,
    "polish_8tags_multiple_choice": 0,
    "polish_8tags_regex": 0,
    "polish_belebele_mc": 0,
    "polish_belebele_regex": 0,
    "polish_cbd_multiple_choice": 0,
    "polish_cbd_regex": 0,
    "polish_dyk_multiple_choice": 0,
    "polish_dyk_regex": 0,
    "polish_klej_ner_multiple_choice": 0,
    "polish_klej_ner_regex": 0,
    "polish_polqa_closed_book": 0,
    "polish_polqa_open_book": 0,
    "polish_polqa_reranking_multiple_choice": 0,
    "polish_ppc_multiple_choice": 0,
    "polish_ppc_regex": 0,
    "polish_psc_multiple_choice": 0,
    "polish_psc_regex": 0
  },
  "config": {
    "model": "hf",
    "model_args": "pretrained=aaditya/Llama3-OpenBioLLM-8B",
    "batch_size": "4",
    "batch_sizes": [],
    "device": "cuda:0",
    "use_cache": null,
    "limit": null,
    "bootstrap_iters": 100000,
    "gen_kwargs": null
  },
  "git_hash": "ef340abf",
  "date": 1715245377.5385582,
  "pretty_env_info": "PyTorch version: 2.3.0+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Fedora Linux 38 (Workstation Edition) (x86_64)\nGCC version: (GCC) 13.2.1 20231011 (Red Hat 13.2.1-4)\nClang version: Could not collect\nCMake version: version 3.28.3\nLibc version: glibc-2.37\n\nPython version: 3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-6.5.6-200.fc38.x86_64-x86_64-with-glibc2.37\nIs CUDA available: True\nCUDA runtime version: Could not collect\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA GeForce RTX 4090\nNvidia driver version: 535.104.05\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nAddress sizes:                      48 bits physical, 48 bits virtual\nByte Order:                         Little Endian\nCPU(s):                             32\nOn-line CPU(s) list:                0-31\nVendor ID:                          AuthenticAMD\nModel name:                         AMD Ryzen 9 7950X 16-Core Processor\nCPU family:                         25\nModel:                              97\nThread(s) per core:                 2\nCore(s) per socket:                 16\nSocket(s):                          1\nStepping:                           2\nCPU(s) scaling MHz:                 30%\nCPU max MHz:                        5881.0000\nCPU min MHz:                        400.0000\nBogoMIPS:                           8982.95\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good amd_lbr_v2 nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba perfmon_v2 ibrs ibpb stibp ibrs_enhanced vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local avx512_bf16 clzero irperf xsaveerptr rdpru wbnoinvd cppc arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif x2avic v_spec_ctrl vnmi avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid overflow_recov succor smca fsrm flush_l1d\nVirtualization:                     AMD-V\nL1d cache:                          512 KiB (16 instances)\nL1i cache:                          512 KiB (16 instances)\nL2 cache:                           16 MiB (16 instances)\nL3 cache:                           64 MiB (2 instances)\nNUMA node(s):                       1\nNUMA node0 CPU(s):                  0-31\nVulnerability Gather data sampling: Not affected\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Not affected\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Mitigation; safe RET, no microcode\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced / Automatic IBRS, IBPB conditional, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.0\n[pip3] torch-tb-profiler==0.4.3\n[pip3] triton==2.3.0\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.3.0                    pypi_0    pypi\n[conda] triton                    2.3.0                    pypi_0    pypi",
  "transformers_version": "4.40.2",
  "upper_git_hash": null
}